\documentclass{article}
\usepackage{amsmath}
\usepackage{indentfirst}
\title{HW1 Word Segmentation Report}
\author{Group BlackRice}
\date{2016/09/27}
\begin{document}
\maketitle
\section{Group Info}
\begin{tabular}{|l|c|r|}
\hline
Group Member& SFU mail\\
\hline
April Wang & ayw7@sfu.ca\\
\hline
Jiahao Ke & jiahaok@sfu.ca\\
\hline
Yu Tang & yta47@sfu.ca\\
\hline
Ruoxin Zhou& ruoxinz@sfu.ca\\
\hline
\end{tabular}
\section{General Idea}

In this project, we present a solution to segment Chinese words and our solution finally hits 93.361 on Leaderboard.

We use some optimizations listed below to improve the naive segmentor based on the pseudo-code provided with HW 1 that uses unigram probabilities:

\begin{itemize}
\item Using the bigram model to score word segmentation candidates. 
\item Adding Good-Turing Algorithm to smooth probability data used by bigram and unigram model
\item Using Jelinek-Mercer Smoothing to improve the bigram model
\item Using Back-off Smoothing to improve accuracy
\item Merging all the digit numbers appeared continuously in a line
\item Trying to recognize proper noun that is not collected in the dictionary but appearing more than twice.
\item Trying to combine single word that is not collected in the dictionary but each single word has a very late prabability to be single in the sentance
\end{itemize}

For Jelinek-Mercer Smoothing, we write a bash script to test how different value of lambda would effect accuracy of final result. We get lambda = 0.17 for the best result.

\section{Solution}

\subsection{Bigram model}
Our language model combines the bigram language model over Chinese words. The input is a sequence of Chinese characters (without word boundaries): $c_0,..., c_n$.

Then define a word as a sequence of characters: $w_i^j$ is a word that spans from character $i$ to character $j$. So one possible word sequence is $w_0^aw_{a+1}^bw_{b+1}^n$. 

\vspace{5pt} 

Score this sequence using Jelinek-Mercer Smoothed bigram probabilities.

\begin{center} $\arg\max_{w_0^iw_{i+1}^j,...,w_{n-k}^n} P_{JM}( w_0^i \mid start) * P_{JM}(w_{i+1}^j \mid w_0^i) * ... * P_{JM}(w_{n-k + 1}^n \mid w_{n-g}^{n-k})$ \end{center}

\subsection{Jelinek-Mercer Smoothing}

In order to get the best Jelinek-Mercer Smoothing, different $\lambda$ should be tried to get the score.
\begin{center} $P_{JM}(w_i \mid w_{i-1}) = \lambda P_b(w_i \mid w_{i-1}) + (1-\lambda)P_w(w_i)$ \end{center}

\subsection{Good-Turing Smoothing}

The bigram probability $P_b$ can be constructed using data in count\_2w.txt, while $P_w$ can be constructed using data in count\_1w.txt.

In order to deal with events that have been observed zero times, we use Good-Turing Smoothing to smooth data in count\_1w.txt and count\_2w.txt.

\begin{center} $r^* = (r + 1) \frac{n_{r+1}}{n_r}$ \end{center}

\subsection{Merging digit numbers}
For merging all the digit numbers, we simply go through the lines that has been segmented and whenever digit numbers appearing continuously, we combine them together.

\subsection{Recognizing proper noun}
For recognizing proper noun, we notice that the testing data is made of by pieces of news. For each news, there would likely be proper noun that appeared more than twice or three times. So for each segmented paragraph, we find those sets of words, while the length of each word is usually one but those sets of words appear continuously for more than twice. We consider those sets of words are likely to be proper noun and we combine them together.

\end{document}
